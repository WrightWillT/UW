{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 558 - Homework 3\n",
    "Will Wright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 1\n",
    "\n",
    "In this exercise, you will implement in **Python** a first version of _your own fast gradient algorithm_ to solve the $\\ell_2^2$-regularized logistic regression problem.  \n",
    "Recall from the lectures that the logistic regression problem writes as:  \n",
    "$$\\min_{\\mathbf{\\beta \\in \\mathbb{R}^d}} F(\\beta):=\\frac{1}{n}\\sum_{i=1}^{n} log(1+exp(-y_ix_i^T\\beta)) + \\lambda\\lVert\\beta\\rVert_2^2 \\tag{1}$$\n",
    "\n",
    "\n",
    "We use here the machine learning convention for the labels that is $y_i\\in${−1, +1}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Fast Gradient\n",
    "The fast gradient algorithm is outlined in Algorithm 1. The algorithm requires a subroutine that computes the gradient for any $\\beta$.  \n",
    "\n",
    "Assume that $d=1$ and $n=1$. The sample is then of size 1 and boils down to just $(x, y)$. The function $F$ writes simply as:  \n",
    "\n",
    "$F(\\beta)=log(1+exp(-yx\\beta))+\\lambda\\beta^2$  \n",
    "\n",
    "**Question**  \n",
    "Compute and write down the gradient $\\nabla F$ of $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  \n",
    "We can break up the sum $log(1+exp(-yx\\beta))+\\lambda\\beta^2$ into two parts: (1) $log(1+exp(-yx\\beta))$ and (2) $\\lambda\\beta^2$ and derive each separately.  \n",
    "\n",
    "(1) $\\frac{\\partial}{\\partial \\beta}log(1+exp(-yx\\beta))$  \n",
    "\n",
    "\n",
    "\n",
    "Use the chain rule with the following functions and their derivatives:  \n",
    "$(f\\circ g \\circ h) = log(g\\circ h)$  \n",
    "$(f\\circ g \\circ h)' = \\frac{1}{g\\circ h} \\cdot (g \\circ h)' $  \n",
    "$(g\\circ h) = 1+exp(-h)$  \n",
    "$(g\\circ h)' = -exp(-h)\\cdot h'$  \n",
    "$h = yx\\beta$  \n",
    "$h' = yx$  \n",
    "  \n",
    "Putting the chains together, we have:  \n",
    "$(f\\circ g \\circ h)' = \\frac{-yx\\cdot exp(-yx\\beta)}{1+exp(-yx\\beta)}$\n",
    "  \n",
    "\n",
    "(2) $\\frac{\\partial}{\\partial \\beta}\\lambda\\beta^2 = 2\\lambda \\beta$  \n",
    "  \n",
    "Finally, we add (1) and (2) to get:  \n",
    "$\\nabla F = \\frac{-yx\\cdot exp(-yx\\beta)}{1+exp(-yx\\beta)} + 2\\lambda \\beta$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**  \n",
    "Assume now that $d>1$ and $n>1$.  Using the previous result and the linearity of differentiation, compute and write down the gradient $\\nabla F(\\beta)$ of $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  \n",
    "We can break up the sum $log(1+exp(-y_ix_i^T\\beta))+\\lambda\\lVert \\beta\\rVert_2^2$ into two parts: (1) $log(1+exp(-y_ix_i^T\\beta))$ and (2) $\\lambda\\lVert \\beta\\rVert_2^2$ and derive each separately.  \n",
    "\n",
    "(1) $\\frac{\\partial}{\\partial \\beta}log(1+exp(-y_ix_i^T\\beta))$  \n",
    "  \n",
    "Use the chain rule with the following functions and their derivatives:  \n",
    "$(f\\circ g \\circ h) = log(g\\circ h)$  \n",
    "$(f\\circ g \\circ h)' = \\frac{1}{(g\\circ h)} \\cdot (g \\circ h)' $  \n",
    "$(g\\circ h) = 1+exp(-h)$  \n",
    "$(g\\circ h)' = -exp(-h)\\cdot h'$  \n",
    "$h = y_ix_i^T\\beta$  \n",
    "$h' = y_ix_i$  \n",
    "  \n",
    "Putting the chains together, we have:  \n",
    "$(f\\circ g \\circ h)' = -y_ix_i \\cdot \\frac{-y_ix_iexp(-y_ix_i^T\\beta)}{1+exp(-y_ix_i^T\\beta)}$\n",
    "  \n",
    "(2) $\\frac{\\partial}{\\partial \\beta}\\lambda\\lVert \\beta\\rVert_2^2 = 2\\lambda \\beta $  \n",
    "  \n",
    "Next, we add (1) and (2) to get:  \n",
    "$\\nabla F = -y_ix_i\\cdot \\frac{exp(-y_ix_i^T\\beta)}{1+exp(-y_ix_i^T\\beta)} + 2\\lambda \\beta$\n",
    "  \n",
    "As as added simplification (to be used in the code below), we can substitude:  \n",
    "$1-p_i = \\frac{exp(-y_ix_i^T\\beta)}{1+exp(-y_ix_i^T\\beta)}$\n",
    "\n",
    "...and arrive at:  \n",
    "$\\nabla F = -y_ix_i(1-p_i) + 2\\lambda \\beta$  \n",
    "\n",
    "If P is an nxn matrix of 0s with diag(P) = $1-p_i$, then we can simply to the final form:  \n",
    "$\\nabla F = -\\frac{1}{n}XYP + 2\\lambda \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the Spam dataset from The Elements of Statistical Learning (You can get it here: https://web.stanford.edu/~hastie/ElemStatLearn/). Standardize the data (i.e., center the features and divide them by their standard deviation, and also change the output labels to +/- 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "from scipy.linalg import eigh as largest_eigh # for calculating the initial eta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "spam = pd.read_csv('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data', \n",
    "                   delim_whitespace=True, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam.iloc[:,0:57]\n",
    "y = spam.iloc[:,-1]\n",
    "y = y.replace(0,-1)\n",
    "\n",
    "# Divide the data into training and test sets. By default, 25% goes into the test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Convert to array\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Standardize data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function _computegrad_ that computes and returns $\\nabla F(\\beta)$ for any $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computegrad(X, y, beta, lam = 0.05):\n",
    "    'Output grad(F) given X, y, beta, and lambda'\n",
    "    ## calc the nxn p-matrix\n",
    "    # start with the diagonal of non-zeros\n",
    "    n = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    P_diag = [0 for x in range(n)] # P is nxn\n",
    "    for i in range(0,n):\n",
    "        yXTbeta = y[i]*(X[i,:].T).dot(beta) # 1x1 * dx1 * 1xd = 1x1\n",
    "        P_diag[i] = 1-(np.exp(yXTbeta))/(1+np.exp(yXTbeta)) \n",
    "    P = np.diagflat(P_diag) # nxn\n",
    "    emp_risk = (-1/n)*(X.T.dot(P).dot(y)).reshape(d,1) # (dxn * nxn) = dxn; dxn * nx1 = dx1\n",
    "    penalty = 2*lam*beta.reshape(d,1) # dx1\n",
    "    grad_F = emp_risk + penalty #dx1 + dx1 = dx1\n",
    "    return grad_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeobj(X, y, beta, lam = 0.05):\n",
    "    'Output F (objective is to minimize) for any given X, y, beta, lam'\n",
    "    n = X.shape[0]\n",
    "    sum_terms = [0 for i in range(n)] # initialize list\n",
    "    for i in range(0, n):\n",
    "        sum_terms[i] = np.log(1+np.exp(-y[i]*X[i,:].T.dot(beta)))\n",
    "    avg_summation = sum(sum_terms)/n\n",
    "    regularization = lam*np.sum(beta**2)\n",
    "    F = avg_summation + regularization\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function _backtracking_ that implements the backtracking rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(X, y, beta, eta, alpha = 0.5, gamma = 0.8):\n",
    "    'Output new eta for a given X, y, beta, eta, alpha, and gamma'\n",
    "    grad = computegrad(X,y,beta)\n",
    "    complete = False\n",
    "    while complete == False:\n",
    "        if computeobj(X, y, beta - eta*grad) < computeobj(X, y, beta) - alpha*eta*np.linalg.norm(grad)**2:\n",
    "            complete = True\n",
    "        else:\n",
    "            eta = eta * gamma\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function _graddescent_ that implements the gradient descent algorithm with the backtracking rule to tune the step-size. The function _graddescent_ calls _computegrad_ and _backtracking_ as subroutines. The function takes as input the initial point, the initial step-size value, and the target accuracy $\\epsilon$. The stopping criterion is $\\lVert \\nabla F \\rVert \\leq \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eta(X, lam = 0.05):\n",
    "    'returns initial step-size based on equation provided in lecture notes'\n",
    "    eigenValues, eigenVectors = np.linalg.eig((1/len(X)) * X.T.dot(X))\n",
    "    L = max(eigenValues) + lam\n",
    "    return 1/L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graddescent(X, y, eta, target = 10e-4, lam = 0.05):\n",
    "    # initialize lists of betas, Fs, and grad(F)s\n",
    "    beta = np.zeros((X.shape[1],1))\n",
    "    betas =[beta] #start with the first beta included\n",
    "    Fs = [computeobj(X,y,beta,lam)] # start with the first F included\n",
    "    grad = computegrad(X,y,beta,lam)\n",
    "    t=0\n",
    "    while (np.linalg.norm(grad)>target):\n",
    "        eta = backtracking(X, y, beta, eta)\n",
    "        beta = beta - eta*grad\n",
    "        betas.append(beta) \n",
    "        F = computeobj(X, y, beta, lam)\n",
    "        Fs.append(F) \n",
    "        grad = computegrad(X, y, beta, lam)\n",
    "        t += 1\n",
    "    return np.array(betas), np.array(Fs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function _fastgradalgo_ that implements the fast gradient algorithm described in Algorithm 1. The function _fastgradalgo_ calls _computegrad_ and _backtracking_ as subroutines. The function takes as input the initial step-size value for the backtracking rule and the target accuracy $\\epsilon$. The stopping criterion is $\\lVert \\nabla F \\rVert \\leq \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastgradalgo(X, y, eta, target = 10e-4, lam = 0.05):\n",
    "    # initialize lists of betas, Fs, and grad(F)s\n",
    "    # identical to standard gradient descent except theta is added and used in the computegrad subroutine\n",
    "    beta = np.zeros((X.shape[1],1))\n",
    "    theta = np.zeros((X.shape[1],1))\n",
    "    betas = [beta] #start with the first beta included\n",
    "    Fs = [computeobj(X,y,beta,lam)] # start with the first F included\n",
    "    grad = computegrad(X,y,theta,lam)\n",
    "    t=0\n",
    "    while (np.linalg.norm(grad) > target):\n",
    "        eta = backtracking(X, y, beta, eta)\n",
    "        beta_prior = copy.copy(beta)\n",
    "        beta = theta - eta * computegrad(X, y, theta, lam)\n",
    "        theta = beta + (t/(t+3))*(beta - beta_prior)\n",
    "        betas.append(beta)\n",
    "        F = computeobj(X, y, beta, lam)\n",
    "        Fs.append(F) \n",
    "        grad = computegrad(X, y, theta, lam)\n",
    "        t += 1\n",
    "    return np.array(betas), np.array(Fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the estimate described in the course to initialize the step-size. Set the target accuracy to $\\epsilon = 10^{−4}$. Run _graddescent_ and _fastgradalgo_ on the training set of the Spam dataset for $\\lambda = 0.1$. Plot the curve of the objective values $F(\\beta_t)$ for both algorithms versus the iteration counter $t$ (use different colors). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_graddescent = graddescent(X_train, y_train, eta = compute_eta(X_train, lam = 0.1), target = 10e-4, lam = 0.1)\n",
    "results_fastdescent = fastgradalgo(X_train, y_train, eta = compute_eta(X_train, lam = 0.1), target = 10e-4, lam = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results_graddescent[1], label = \"gradient descent\")\n",
    "plt.plot(results_fastdescent[1], label = \"fast gradient descent\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.ylabel('Log-scaled F-Value')\n",
    "plt.xlabel('Iteration (t)')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 99 iterations while fast gradient descent took only 57\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient descent took\",results_graddescent[0].shape[0],\n",
    "      \"iterations while fast gradient descent took only\",results_fastdescent[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote by $\\beta_T$ the final iterate of your fast gradient algorithm. Compare $\\beta_T$ to the $\\beta^{*}$ found by scikit-learn. Compare the objective value for $\\beta_T$ to the one for $\\beta^{*}$. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_T = results_fastdescent[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Will\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10.0, class_weight=None, dual=False, fit_intercept=False,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(penalty='l2', dual=False, C=1/0.1, tol=10e-4, fit_intercept=False, solver='saga')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fastgrad_betas</th>\n",
       "      <th>sklearn_betas</th>\n",
       "      <th>beta_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029713</td>\n",
       "      <td>-0.044628</td>\n",
       "      <td>0.074341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.022302</td>\n",
       "      <td>-0.113361</td>\n",
       "      <td>0.091058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.086928</td>\n",
       "      <td>0.097073</td>\n",
       "      <td>-0.010146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058870</td>\n",
       "      <td>0.484505</td>\n",
       "      <td>-0.425635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.175014</td>\n",
       "      <td>0.437646</td>\n",
       "      <td>-0.262631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.120720</td>\n",
       "      <td>0.138895</td>\n",
       "      <td>-0.018175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.295621</td>\n",
       "      <td>1.440964</td>\n",
       "      <td>-1.145342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.131526</td>\n",
       "      <td>0.224513</td>\n",
       "      <td>-0.092986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.121934</td>\n",
       "      <td>0.174459</td>\n",
       "      <td>-0.052525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.050707</td>\n",
       "      <td>0.032233</td>\n",
       "      <td>0.018473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.138491</td>\n",
       "      <td>0.043833</td>\n",
       "      <td>0.094658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.038123</td>\n",
       "      <td>-0.108946</td>\n",
       "      <td>0.070823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.056174</td>\n",
       "      <td>-0.018827</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.032547</td>\n",
       "      <td>0.028285</td>\n",
       "      <td>0.004262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.108993</td>\n",
       "      <td>0.450038</td>\n",
       "      <td>-0.341045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.197340</td>\n",
       "      <td>0.466147</td>\n",
       "      <td>-0.268807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.156930</td>\n",
       "      <td>0.361310</td>\n",
       "      <td>-0.204381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.118656</td>\n",
       "      <td>0.094175</td>\n",
       "      <td>0.024481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.115662</td>\n",
       "      <td>0.213679</td>\n",
       "      <td>-0.098017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.129245</td>\n",
       "      <td>0.774318</td>\n",
       "      <td>-0.645073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.235357</td>\n",
       "      <td>0.296203</td>\n",
       "      <td>-0.060846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.101586</td>\n",
       "      <td>0.548833</td>\n",
       "      <td>-0.447247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.250822</td>\n",
       "      <td>1.433576</td>\n",
       "      <td>-1.182753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.153630</td>\n",
       "      <td>0.346286</td>\n",
       "      <td>-0.192656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.148467</td>\n",
       "      <td>-1.130345</td>\n",
       "      <td>0.981878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.104621</td>\n",
       "      <td>-0.432171</td>\n",
       "      <td>0.327550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.120967</td>\n",
       "      <td>-0.860777</td>\n",
       "      <td>0.739810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.034818</td>\n",
       "      <td>0.187117</td>\n",
       "      <td>-0.221935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.050048</td>\n",
       "      <td>-0.191855</td>\n",
       "      <td>0.141808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.075760</td>\n",
       "      <td>-0.198019</td>\n",
       "      <td>0.122259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.032925</td>\n",
       "      <td>-0.112610</td>\n",
       "      <td>0.079685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.013644</td>\n",
       "      <td>0.078841</td>\n",
       "      <td>-0.092485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.079288</td>\n",
       "      <td>-0.402333</td>\n",
       "      <td>0.323044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.016171</td>\n",
       "      <td>-0.044328</td>\n",
       "      <td>0.028157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.056882</td>\n",
       "      <td>-0.219963</td>\n",
       "      <td>0.163081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.003802</td>\n",
       "      <td>0.287956</td>\n",
       "      <td>-0.291758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.080537</td>\n",
       "      <td>-0.084026</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.022007</td>\n",
       "      <td>-0.001753</td>\n",
       "      <td>-0.020254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.057730</td>\n",
       "      <td>-0.178839</td>\n",
       "      <td>0.121109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.015178</td>\n",
       "      <td>-0.062182</td>\n",
       "      <td>0.077360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.052173</td>\n",
       "      <td>-0.192387</td>\n",
       "      <td>0.140214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.096526</td>\n",
       "      <td>-0.609186</td>\n",
       "      <td>0.512661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.058606</td>\n",
       "      <td>-0.096795</td>\n",
       "      <td>0.038189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.066615</td>\n",
       "      <td>-0.352138</td>\n",
       "      <td>0.285523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.110694</td>\n",
       "      <td>-0.549366</td>\n",
       "      <td>0.438672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.677397</td>\n",
       "      <td>0.567148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.044859</td>\n",
       "      <td>-0.152278</td>\n",
       "      <td>0.107419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-0.059784</td>\n",
       "      <td>-0.385496</td>\n",
       "      <td>0.325711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.068232</td>\n",
       "      <td>-0.430084</td>\n",
       "      <td>0.361852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.032757</td>\n",
       "      <td>-0.076026</td>\n",
       "      <td>0.043268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.028192</td>\n",
       "      <td>-0.161174</td>\n",
       "      <td>0.132982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.160121</td>\n",
       "      <td>0.363286</td>\n",
       "      <td>-0.203164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.256384</td>\n",
       "      <td>1.580060</td>\n",
       "      <td>-1.323676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.060360</td>\n",
       "      <td>0.566979</td>\n",
       "      <td>-0.506619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.067043</td>\n",
       "      <td>0.335321</td>\n",
       "      <td>-0.268278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.133637</td>\n",
       "      <td>0.907480</td>\n",
       "      <td>-0.773844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.166404</td>\n",
       "      <td>0.443002</td>\n",
       "      <td>-0.276598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fastgrad_betas  sklearn_betas  beta_diff\n",
       "0         0.029713      -0.044628   0.074341\n",
       "1        -0.022302      -0.113361   0.091058\n",
       "2         0.086928       0.097073  -0.010146\n",
       "3         0.058870       0.484505  -0.425635\n",
       "4         0.175014       0.437646  -0.262631\n",
       "5         0.120720       0.138895  -0.018175\n",
       "6         0.295621       1.440964  -1.145342\n",
       "7         0.131526       0.224513  -0.092986\n",
       "8         0.121934       0.174459  -0.052525\n",
       "9         0.050707       0.032233   0.018473\n",
       "10        0.138491       0.043833   0.094658\n",
       "11       -0.038123      -0.108946   0.070823\n",
       "12        0.056174      -0.018827   0.075000\n",
       "13        0.032547       0.028285   0.004262\n",
       "14        0.108993       0.450038  -0.341045\n",
       "15        0.197340       0.466147  -0.268807\n",
       "16        0.156930       0.361310  -0.204381\n",
       "17        0.118656       0.094175   0.024481\n",
       "18        0.115662       0.213679  -0.098017\n",
       "19        0.129245       0.774318  -0.645073\n",
       "20        0.235357       0.296203  -0.060846\n",
       "21        0.101586       0.548833  -0.447247\n",
       "22        0.250822       1.433576  -1.182753\n",
       "23        0.153630       0.346286  -0.192656\n",
       "24       -0.148467      -1.130345   0.981878\n",
       "25       -0.104621      -0.432171   0.327550\n",
       "26       -0.120967      -0.860777   0.739810\n",
       "27       -0.034818       0.187117  -0.221935\n",
       "28       -0.050048      -0.191855   0.141808\n",
       "29       -0.075760      -0.198019   0.122259\n",
       "30       -0.032925      -0.112610   0.079685\n",
       "31       -0.013644       0.078841  -0.092485\n",
       "32       -0.079288      -0.402333   0.323044\n",
       "33       -0.016171      -0.044328   0.028157\n",
       "34       -0.056882      -0.219963   0.163081\n",
       "35       -0.003802       0.287956  -0.291758\n",
       "36       -0.080537      -0.084026   0.003488\n",
       "37       -0.022007      -0.001753  -0.020254\n",
       "38       -0.057730      -0.178839   0.121109\n",
       "39        0.015178      -0.062182   0.077360\n",
       "40       -0.052173      -0.192387   0.140214\n",
       "41       -0.096526      -0.609186   0.512661\n",
       "42       -0.058606      -0.096795   0.038189\n",
       "43       -0.066615      -0.352138   0.285523\n",
       "44       -0.110694      -0.549366   0.438672\n",
       "45       -0.110249      -0.677397   0.567148\n",
       "46       -0.044859      -0.152278   0.107419\n",
       "47       -0.059784      -0.385496   0.325711\n",
       "48       -0.068232      -0.430084   0.361852\n",
       "49       -0.032757      -0.076026   0.043268\n",
       "50       -0.028192      -0.161174   0.132982\n",
       "51        0.160121       0.363286  -0.203164\n",
       "52        0.256384       1.580060  -1.323676\n",
       "53        0.060360       0.566979  -0.506619\n",
       "54        0.067043       0.335321  -0.268278\n",
       "55        0.133637       0.907480  -0.773844\n",
       "56        0.166404       0.443002  -0.276598"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_comparison = pd.DataFrame(np.concatenate((beta_T,classifier.coef_.T),axis=1), columns = ['fastgrad_betas','sklearn_betas'])\n",
    "beta_comparison['beta_diff'] = beta_comparison['fastgrad_betas'] - beta_comparison['sklearn_betas']\n",
    "beta_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45068687])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastgrad_obj = results_fastdescent[1][-1]\n",
    "fastgrad_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97786854])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_obj = computeobj(X_train, y_train, classifier.coef_.T)\n",
    "sklearn_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta_T$ is smaller than $\\beta_*$.  This could possibly be due to the type of solver being used or a different alpha/gamma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cross-validation on the training set of the Spam dataset using _scikit-learn_ to find the optimal value of $\\lambda$. Run _graddescent_ and _fastgradalgo_ to optimize the objective with that value of $\\lambda$. Plot the curve of the objective values $F(\\beta_t)$ for both algorithms versus the iteration counter $t$. Plot the misclassification error on the training set for both algorithms versus the iteration counter $t$. Plot the misclassification error on the test set for both algorithms versus the iteration counter $t$. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Come back to this]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8992180712423979"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we estimate the regression coefficients in a logistic regression model by minimizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F(\\beta):=\\frac{1}{n}\\sum_{i=1}^{n} log(1+exp(-y_ix_i^T\\beta)) + \\lambda\\lVert\\beta\\rVert_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for a particular value of $\\lambda$. For parts (a) through (b), indicate which of (i) through (v) is correct. Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;(a) As we increase $\\lambda$ from 0, the misclassification error on the test set will:  \n",
    "&emsp;&emsp;(i) Increase initially and then eventually start decreasing in an inverted U shape.  \n",
    "&emsp;&emsp;(ii) Decrease initially and then eventually start increasing in a U shape.  \n",
    "&emsp;&emsp;(iii) Steadily increase.  \n",
    "&emsp;&emsp;(iv) Steadily decrease.  \n",
    "&emsp;&emsp;(v) Remain constant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: (ii) Decrease initially and then eventually start increasing in a U-shape.  This is because the misclassification error  is an additive function of bias and variance where bias increases in an S-shape with $\\lambda$ and variance decreases in an S-shape with $\\lambda$. Due to this bias-variance trade-off, the error forms a U-shape with a minimum point for the optimal $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;(b) Repeat (a) for the misclassification error on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Come back to this]** I think this question is asking for a chart showing MSE, bias, and variance with $\\lambda$ on the x-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 3\n",
    "In this exercise, you will use Amazon Web Services (AWS) to run a nearest neighbors algorithm on data from the Sloan Digital Sky Survey.  The goals of this exercise are:  \n",
    "&emsp;1. To teach you how to use AWS's Elastic Compute Cloud (EC2) and Simple Storage Service (S3)  \n",
    "&emsp;2. To show you that computing with GPUs can be much faster than computing with CPUs  \n",
    "&emsp;3. To demonstrate a fast algorithm for computing nearest neighbors (from [1])  \n",
    "\n",
    "**Background:** The Sloan Digital Sky Survey (SDSS) has gathered data on many objects in the sky. The data set you will use in this assignment consists of (1) a “training” set of pho- tometric data on known astronomical objects; and (2) a “test” set of additional photometric data on more objects. Astronomers often need to determine which objects in a new data set are worth further examination because their telescope time is very limited. By finding the nearest neighbors in the training set to each object in the test set, they can tell whether the object is interesting or not. For example, if an object in the test set is closest to sun-like stars, then they might find that one very boring and not follow up on it.  \n",
    "\n",
    "**Instructions:** Following the AWS tutorial for help, do the following:  \n",
    "&emsp;1. Create a p2.xlarge spot instance with the “Deep Learning AMI (Ubuntu) Version 22.0” AMI. If this instance type doesn’t work, try another one of the gpu instances. You need to use a GPU instance for this assignment.  \n",
    "\n",
    "&emsp;2. Install Swig and bufferkdtree on your instance.  \n",
    "\n",
    "&emsp;3. On your instance, run the astronomy example from here https://github.com/gieseke/bufferkdtree/tree/master/examples and save all of the output to the file “out- put.txt”. **Hint:** You will need to transfer both astronomy.py and generate.py on EC2. In addition, in the file astronomy.py you will need to change the line  \n",
    "plat dev ids = {0:[0 ,1 ,2 ,3]} to  \n",
    "plat dev ids = {0:[0]}  \n",
    "The data will automatically download the first time you run it **after user input**. Therefore, do not try to save the output to a file via > the first time you run it or else it will hang indefinitely. Either use the tee command (https://en.wikipedia. org/wiki/Tee_(command)), run the script once before using > or copy and paste the output to a file. If you are still having issues with it hanging when saving directly to a file, check the output file. It’s possible that it finished writing out the output.  \n",
    "\n",
    "&emsp;4. Transfer the file output.txt to an S3 bucket.  \n",
    "\n",
    "&emsp;Go t the S3 interface https://console.aws.amazon.com/s3/home?region=us-east-1 and make that file public.  \n",
    "\n",
    "&emsp;In your homework submission please include the following:  \n",
    "\n",
    "&emsp;&emsp;(a) The fitting and testing times on both the CPU and GPU versions. You can find these in the output.  \n",
    "\n",
    "&emsp;&emsp;(b) The url from the previous step (Check to make sure you successfully made it public!)  \n",
    "\n",
    "&emsp;&emsp;(c) A statement of any problems you encountered during this exercise and how you overcame them (or if you didn’t).  \n",
    "\n",
    "&emsp;&emsp;(d) How long it took you to complete this exercise (for our reference–we’re not grading you on how long it took).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 4\n",
    "Read the announcement “Data Competition, Part 1” released on Canvas. We strongly recommend you perform this task on AWS. You will use your own $\\ell_2^2$_-regularized logistic regression_ for this exercise. After completing this exercise, submit your predictions to the data competition Kaggle website.  \n",
    "\n",
    "&emsp;Download the data for the Kaggle competition. Run the script extract features.py to extract features from the images. This script was written in Python 3 and depends on the library PyTorch.  \n",
    "\n",
    "&emsp;Pick two classes of your choice from the dataset. Train an $\\ell_2^2$_-regularized logistic regression_ classifier on the training set using your own fast gradient algorithm with $\\lambda$ = 1. Be sure to use the features you generated above rather than the raw image features. Plot, with different colors, the _misclassification error_ on the training set and on the validation set vs iterations.  \n",
    "\n",
    "&emsp;Find the value of the regularization parameter $\\lambda$ using cross-validation; you may use scikit-learn’s built-in functions for this purpose. Train an $\\ell_2^2$_-regularized logistic regression_ classifier on the training set using your own fast gradient algorithm with that value of $\\lambda$ found by cross-validation. Plot, with different colors, the misclassification error on the training set and on the validation set vs iterations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Exercise 5 (Optional)\n",
    "It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting.  \n",
    "\n",
    "Suppose that $n = 2, p = 2, x_11 = x_12, x_21 = x_22$. Furthermore, suppose that $y_1 + y_2 = 0$ and $x_11 + x_21 = 0$ and $x_12 + x_22 = 0$, so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: $\\hat{β}_0 = 0$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;(a) Write out the ridge regression optimization problem in this setting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;(b) Argue that in this setting, the ridge coefficient estimates satisfy $\\beta_1 = \\beta_2$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
