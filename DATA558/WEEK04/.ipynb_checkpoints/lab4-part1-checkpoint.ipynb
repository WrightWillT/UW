{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Principal Component Analysis </center>\n",
    "<center> &copy; Corinne Jones (TA) and Zaid Harchaoui (MI) </center>\n",
    "<center> DATA 558, Spring 2019, University of Washington </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we shall explore principal component analysis and and feature transformations. By the end of this lab, you should:\n",
    "- Understand what principal component analysis does\n",
    "- Understand waht feature standardization does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Principal component analysis (PCA)\n",
    "Principal component analysis (PCA) is a method that is used to transform a set of variables into a smaller set of variables that explain most of the variability in the original dataset. PCA has no response variable, so it is an \"unsupervised\" learning method. \n",
    "\n",
    "Why would we want to use such a method? It can be useful for data compression and visualization, and the results can also be used in other downstream tasks, such as regression, classification, and clustering. Using the results from PCA instead of the original features often leads to less noisy results, as the first few principal components  tend to contain the signal in data (as opposed to the noise). One downside is that after applying PCA, the regression and classification coefficients often become uninterpretable.\n",
    "\n",
    "In this lab we'll see how to run PCA on a dataset of faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 PCA overview\n",
    "The goal of PCA is to project a dataset consisting of observations $x_1,\\dots, x_n \\in \\mathbb{R}^D$ onto a lower dimensional subspace of dimension $M$ such that we maximize the variance of the projected data. An equivalent way of viewing PCA is that we are projecting the data onto a subspace that is \"closest\" to the observations. If we try to reconstruct the original data from the data projected using PCA, we will get the \"best\" possible reconstruction. All of this may sound confusing, so let's look at a couple of pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps  # PIL is the Python Imaging Library\n",
    "import requests\n",
    "\n",
    "image_url1 = 'http://alexhwilliams.info/itsneuronalblog/img/pca/pca_classic.png'\n",
    "image_url2 = 'http://alexhwilliams.info/itsneuronalblog/img/pca/pca_two_views.png'\n",
    "for i in range(0, 2):\n",
    "    image_url = image_url1 if i==0 else image_url2\n",
    "    response = requests.get(image_url)\n",
    "    pca_figure = Image.open(BytesIO(response.content))\n",
    "    if i == 0:\n",
    "        fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    fig = ax.imshow(np.asarray(pca_figure), aspect='auto')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first picture, we have three-dimensional data ($D=3$) and each blue point is an observation. When using PCA to reduce this data to two dimensions, we find the plane that is closest to the data. Equivalently, we find the directions that capture the largest amount of the variance in the data and such that the directions are orthogonal. The red arrows represent the directions with the largest variance.\n",
    "\n",
    "In the picure at the bottom there are $n=16$ observations $x_1,\\dots, x_{16}$ (the gray points), each with $D=2$ dimensions. The image shows the results of running PCA on these observations. The direction with the maximum variation in the data is denoted by the black line. Hence, if we project the data down to one dimension, we will maximize the variance of the projected data if we project along this axis. Equivalently, the distance between the points and a subspace (a line) is at a minimum if we choose the line to be the black line in the picture. The red points are the values of the observations after they were projected onto the first principal component $v_1$. \n",
    "\n",
    "A note on terminology before continuing: Some references call $v_1$ a principal component, whereas others call $Xv_1$ a principal component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's briefly look at the math behind PCA. This section is based off of the notes from here: \n",
    "http://www.cs.columbia.edu/~stratos/research/pca_cca.pdf\n",
    "\n",
    "Let $X\\in\\mathbb{R}^D$ be a random variable. Without loss of generality assume it has a mean of zero. The goal is to find a lower dimensional random variable $X'=(X'_1, \\dots, X'_M)\\in\\mathbb{R}^M$ such that each successive element $X_i'\\in\\mathbb{R}$ has as much variance as possible and is uncorrelated with the previous values. In other words, for $i=1,\\dots, M$, we solve the following problem:\n",
    "$$ X_i' = \\arg\\max_{\\Psi}\\text{Var}(\\Psi)$$\n",
    "subject to the constraint that $\\Psi=v^TX$ for some vector $v\\in\\mathbb{R}^D$ with $\\|v\\|^2=1$ and such that \n",
    "$$ \\text{Corr}(\\Psi, X'_j) = 0$$\n",
    "for $j=1,\\dots, i-1$. We require $\\|v\\|^2=1$ because otherwise we could make $\\text{Var}(v^TX)$ arbitrarily large. \n",
    "\n",
    "Let $v_1,\\dots, v_M$ be the optimal values of $v$ from above. The PCA projections are given by $\\Psi = V^TX$, where $V=[v_1,\\dots, v_M]$.\n",
    "\n",
    "It turns out that $v_1,\\dots, v_M$ are the eigenvectors corresponding to the $M$ largest eigenvalues of the covariance matrix of $X$, $E[XX^T]$. Moreover, the amount of variance explained by each component is the eigenvalue corresponding to that component.\n",
    "\n",
    "In practice we don't have a random variable; we have samples $x_1,\\dots, x_n$. Therefore, we estimate the covariance matrix with the empirical covariance matrix. The resulting PCA algorithm proceeds as follows:\n",
    "> INPUT: Samples $x_1,\\dots, x_n\\in\\mathbb{R}^D$ and an integer $M\\leq D$\n",
    "> 1. Compute $\\hat\\mu\\in\\mathbb{R}^D$ where $\\hat\\mu_i = \\frac{1}{n}\\sum_{k=1}^n x_{k,i}$ for $i=1,\\dots,D$. Let $Z=[z_1,\\dots,z_n]\\in\\mathbb{R}^{D\\times N}$ where\n",
    "> $$z_k = x_k - \\hat\\mu \\:.$$\n",
    "> 2. Set $\\hat C_{XX} = \\frac{1}{N}ZZ^T\\in\\mathbb{R}^{D\\times D}$.\n",
    "> 3. Compute $\\hat V_M = \\text{Matrix of eigenvectors corresponding to the top $M$ eigenvalues of $\\hat C_{XX}$}$.\n",
    "> 4. Compute ${x}'_{1},\\dots, x'_N\\in\\mathbb{R}^M$, where\n",
    "> $$ x'_{k} = \\hat V_m^T z_k \\:.$$  \n",
    ">\n",
    "> OUTPUT: Samples $x'_1,\\dots, x'_n\\in\\mathbb{R}^M$\n",
    "\n",
    "To convert a projected point $x'_k$ to the original space, we need to use apply $\\hat V_M$, to $x'_k$ and add the mean of the $x_i$'s:\n",
    "$$ \\hat x_k = \\hat V_M x'_k + \\hat\\mu$$\n",
    "($\\hat V_M$ is the same as the pseudo-inverse of $\\hat V_M^T$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main takeaways from above are:\n",
    "1. The eigenvectors of the empirical covariance matrix corresponding to the top $M$ eigenvalues tell us how to project the data onto an $M$-dimensional space.\n",
    "2. The variance of the $k$th coordinate of the projected data is given by the $k$th largest eigenvalue.\n",
    "\n",
    "If you want to read more about PCA, two nice references are:\n",
    "- Section 12.1 in Pattern Recognition and Machine Learning by Christopher Bishop\n",
    "- This note by Karl Stratos: http://www.cs.columbia.edu/~stratos/research/pca_cca.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Example data\n",
    "For this part of the lab we are going to use images of people's faces from the Faces in the Wild dataset (http://vis-www.cs.umass.edu/lfw/ ). The data may take several minutes to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "faces = fetch_lfw_people(min_faces_per_person=30) \n",
    "print('People in the dataset:', faces.target_names)  # Images of 34 different people\n",
    "print('Dimensions of data:', faces.images.shape)  # 2370 images, each of which is 62x47 pixels\n",
    "print('Dimension of unraveled data:', faces.data.shape)    # Each image is unraveled to a vector of size 2914"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the data before doing anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = faces.data\n",
    "print('Mean of first ten features: \\n', X.mean(axis=0)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variance of first ten features:\\n', X.var(axis=0)[0:10,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Min of first ten features:', X.min(axis=0)[0:10])\n",
    "print('Max of first ten features:', X.max(axis=0)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some of the faces\n",
    "fig, ax = plt.subplots(1, 10, figsize=(20, 5),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "start = 700\n",
    "for i in range(start, start+10):\n",
    "    ax[i-start].imshow(X[i, :].reshape(62, 47), cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** Compute the \"average\" face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one feature has a much higher variance than the other features, it will have a very large influence on the principal components. If we don't want this to occur, then prior to performing PCA we should standardize the variables to have a mean of zero and a standard deviation of 1. In our case we will not divide by the standard deviation because the features are already normalized to be between 0 and 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 PCA with scikit-learn\n",
    "Let's try running PCA with scikit-learn. By default, scikit-learn will use the randomized PCA version since the dimensions of the data are so large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** Use scikit-learn's `PCA` to run PCA on our data with 200 components. Call the PCA object `pca` and recall that our data is stored in `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PCA class has several important attributes and methods. The attributes that are relevant to us are:\n",
    "- components_ : \"Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained\\_variance\\_.\"\n",
    "- explained_variance\\_ratio\\_ : \"Percentage of variance explained by each of the selected components.\"\n",
    "\n",
    "Let's look at the first few principal components. Recall that the first principal component $v_1$ is the direction that captures the most variance. The second principal component $v_2$ is the direction that is orthogonal to the first direction and captures as much variance as possible, etc. Each principal component is orthogonal to the previous components. Normally we can't visualize the principal components because they're high-dimensional vectors. But with images we can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we visualize the eigenvectors (here called \"eigenfaces\"), remember that they're just vectors:\n",
    "print('First principal component:', pca.components_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector is the analogue of the eigenvector [1,1] that we found in the simple example from Section 2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** Visualize the principal components by substituting the relevant code into the last line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few principal components here (top row) capture the differences in the lighting across the images. Others seem to capture the variation in the eyes, noses, and facial hair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many principal components?\n",
    "One question we need to answer is \"How many principal components should we use?\" We want enough principal components to capture most of the variation in the data, but not too many. This tends to be a problem- and data-specific question. If we're performing PCA for use in a supervised learning task, then we can use cross-validation to choose the best number of components. Other ways that are ad-hoc include:\n",
    "- Choosing the smallest number of components such that a certain percentage of the variance is retained (e.g., 95%)\n",
    "- Looking at what's called a \"scree plot\", which is a plot of the percent of variance captured by each principal component. The \"elbow\" method looks at where there plot goes from having a steep slope to being relatively flat and takes the point just before that occurs to be the number of principal components to use.\n",
    "\n",
    "Let's see how we could go about using the latter two methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** How much variance is explained by the first 25 principal components? How many components are necessary to capture 95% of the variance in the data? \n",
    "Hint: `pca.explained_variance_ratio_` contains the percent of variance explained by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Variance explained by first 25:', sum(pca.explained_variance_ratio_[0:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(min(np.where(cumsum > 0.95)[0]+1), 'components required to capture 95% of the variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** Plot the percent of variance explained by each component vs. the component number. Based on the elbow method heuristic, how many principal components would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we keep only the first 25 principal components. We'll first project the images down to 25 dimensions (the projected space). There are three ways to do this:\n",
    "- \"By hand\", using the fact that the projected values are just the matrix of eigenvectors times the matrix of features\n",
    "- Using the `transform()` method in sklearn after setting the PCA algorithm to find the top 25 components\n",
    "- With the `fit_transform()` method in sklearn after setting the PCA algorithm to find the top 25 components\n",
    "\n",
    "With the first method, we don't need to re-run the PCA algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** Compute the projections of the images to 25 dimensions \"by-hand\", i.e., without using scikit-learn's `transform()` or `fit_transform()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** Use scikit-learn's `fit_transform()` method to check your result from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there's not a very meaningful way to examine the projected faces. However, what we can examine is the reconstructed images when we transform the projected images back to the original space. To transform them back we can again either do it \"by hand\" or using scikit-learn. Recall that to get back to the original data, we use the equation  \n",
    "PCA reconstruction = Projected Values$\\times$Eigenvectors^T+Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** Compute the reconstructed faces \"by-hand\" using the projection the projected values you already computed in Exercise 14, along with the principal components computed by scikit-learn, and mu, which you may have stored in Exercise 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** Compute the reconstructed faces using scikit-learn's `inverse_transform()` method. Compare your result to that of previous exercise. The answers will be close, but not exactly the same because inverse_transform isn't the exact inverse transform of transform when using the randomized option, as noted on this page http://scikit-learn.org/stable/modules/decomposition.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine a few of the reconstructed faces! Each reconstructed face is created from the projected faces using the \"eigenfaces\" (the eigenvectors). More specifically, each reconstructed face is a linear combination of the eigenfaces. It's kind of amazing that after projecting to a 25-dimensional space we are able to reconstruct this much of the original images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise ** In the code below substitute the variable containing the reconstructed faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization and Transformation\n",
    "There are many useful ways in which you can transform your features, and how you should transform them depends on the context. In this section we will examine two common transformations and examples of when to use them. Let $x_1,\\dots, x_n$ denote the inputs, $y_1,\\dots, y_n$ denote the outputs (if there are any), and let $x_{ij}$ denote the $j$th feature of observation $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Standardization\n",
    "When we standardize a feature $j$ we compute $$ \\frac{x_{ij} - \\mu_j}{sd_j},$$\n",
    "where $\\mu_j$ is the mean of the $j$th feature across the observations and $sd_j$ is the corresponding standard deviation. Standardization is very commonly used because it puts all features on the same scale. This is useful for several reasons, including:\n",
    "- In models with penalties on the coefficients, all features should be treated \"equally\", which can be done if the variables are on the same scale.\n",
    "- Optimization algorithms tend to converge faster when the features are on the same scale.\n",
    "- Models based on distances between features that are multi-dimensional tend to have lower error when the features are on the same scale.\n",
    "\n",
    "Let's consider a very small nearest neighbors example. Suppose you have data on five people consisting of their wages and whether or not their employer provides them with a healthcare plan. The data for each person is summarized in the dataframe below. You want to find the person whose characteristics are most similar to a sixth person, whose healthplan status and wages are [1, 100005]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'healthcare': [0, 1, 1, 0, 1], 'wages': [90000, 95000, 110000, 100010, 100015], })\n",
    "X = df.as_matrix()\n",
    "new_person = np.array([1, 100005]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we use 1-NN without standardizing the data. Then what do we find? That the fourth person is closest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=1).fit(X)\n",
    "nearest = nn.kneighbors(new_person)\n",
    "print('Distance to nearest neighbor and index of nearest neighbor:', nearest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in reality, having a healthplan is much more valuable than getting paid $5 more. Therefore, we would want that the fifth person is most similar. We can obtain this outcome if we standardize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "new_person = scaler.transform(new_person)\n",
    "nn = NearestNeighbors(n_neighbors=1).fit(X)\n",
    "nearest = nn.kneighbors(new_person.reshape(1, -1))\n",
    "print('Distance to nearest neighbor and index of nearest neighbor:', nearest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
