{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 558 Midterm\n",
    "\n",
    "Will Wright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Compute the gradient $\\nabla F(\\beta)$ where the objective is:\n",
    "$$\\min_{\\mathbf{\\beta \\in \\mathbb{R}^d}} F(\\beta):=\\frac{1}{n}\\sum_{i=1}^{n} \\frac{1}{\\rho}log(1+exp(-\\rho y_ix_i^T\\beta)) + \\lambda\\lVert\\beta\\rVert_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**  \n",
    "Start by moving the scalar $\\frac{1}{\\rho}$ outside the summation:  \n",
    "$F(\\beta)=\\frac{1}{n\\rho}\\sum_{i=1}^{n} log(1+exp(-\\rho y_ix_i^T\\beta)) + \\lambda\\lVert\\beta\\rVert_2^2$\n",
    "\n",
    "Next, break up into two terms:  \n",
    "(1) $\\frac{1}{n\\rho}\\sum_{i=1}^{n} log(1+exp(-\\rho y_ix_i^T\\beta))$  \n",
    "(2) $\\lambda\\lVert \\beta\\rVert_2^2$ \n",
    "\n",
    "Find the derivative of the first term:  \n",
    "(1) $\\frac{\\partial}{\\partial \\beta}\\Big[\\frac{1}{n\\rho}\\sum_{i=1}^{n} log(1+exp(-\\rho y_ix_i^T\\beta))\\Big]$  \n",
    "  \n",
    "> Move the constant and summation to the outside of the derivative: \n",
    "$\\frac{1}{n\\rho}\\sum_{i=1}^{n}\\frac{\\partial}{\\partial \\beta}\\Big[ log(1+exp(-\\rho y_ix_i^T\\beta))\\Big]$ \n",
    "  \n",
    "> Use the chain rule with the following functions and their derivatives:  \n",
    "$(f\\circ g \\circ h) = log(g\\circ h)$  \n",
    "$(f\\circ g \\circ h)' = \\frac{1}{(g\\circ h)} \\cdot (g \\circ h)' $  \n",
    "$(g\\circ h) = 1+exp(-h)$  \n",
    "$(g\\circ h)' = -exp(-h)\\cdot h'$  \n",
    "$h = \\rho y_ix_i^T\\beta$  \n",
    "$h' = \\rho y_ix_i$  \n",
    "  \n",
    "> Putting the chains together, we have:  \n",
    "$(f\\circ g \\circ h)' = -\\rho y_ix_i \\cdot \\frac{exp(- \\rho y_ix_i^T\\beta)}{1+exp(-\\rho y_ix_i^T\\beta)}$\n",
    "  \n",
    "> Re-apply constant scalar and summation:  \n",
    "$=\\frac{1}{n\\rho}\\sum_{i=1}^{n}-\\rho y_ix_i \\cdot \\frac{exp(- \\rho y_ix_i^T\\beta)}{1+exp(-\\rho y_ix_i^T\\beta)}$\n",
    "\n",
    "> Move the constant $-\\rho$ outside the summation and cancel it out:\n",
    "$=-\\frac{1}{n}\\sum_{i=1}^{n}y_ix_i \\cdot \\frac{exp(- \\rho y_ix_i^T\\beta)}{1+exp(-\\rho y_ix_i^T\\beta)}$\n",
    "\n",
    "Next, derive the second term:  \n",
    "(2) $\\frac{\\partial}{\\partial \\beta}\\lambda\\lVert \\beta\\rVert_2^2$  \n",
    "  \n",
    "> Move the constant $\\lambda$ outside the derivation and convert to matrix form:  \n",
    "$ =\\lambda \\frac{\\partial}{\\partial \\beta}\\beta^T \\beta$\n",
    "  \n",
    "> Multiply by the identity matrix $I$:  \n",
    "$ =\\lambda \\frac{\\partial}{\\partial \\beta}\\beta^T I \\beta$\n",
    "  \n",
    "> Apply the property $\\frac{\\partial}{\\partial x}x^T Ax = (A+A^T)x$:  \n",
    "$ =\\lambda (I + I^T)\\beta$  \n",
    "  \n",
    "> Given that $(I + I^T)$ is simply a scalar of 2 for matrices, we can simplify:  \n",
    "\n",
    "> $=2\\lambda \\beta $  \n",
    "  \n",
    "Next, we add (1) and (2) to get:  \n",
    "$\\nabla F(\\beta) = -\\frac{1}{n}\\sum_{i=1}^{n}y_ix_i \\cdot \\frac{exp(- \\rho y_ix_i^T\\beta)}{1+exp(-\\rho y_ix_i^T\\beta)} + 2\\lambda \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Consider the Spam dataset from The Elements of Statistical Learning. Standardize the data, if you have not done so already. Be sure to use the training and test splits from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "import sklearn.preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "spam = pd.read_table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data', \n",
    "                   delim_whitespace=True, header = None)\n",
    "test_indicator = pd.read_table('https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.traintest',\n",
    "                         delim_whitespace=True, header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(spam)[:, 0:-1]\n",
    "y = np.asarray(spam)[:, -1]*2 - 1\n",
    "test_indicator = np.array(test_indicator).T[0]\n",
    "\n",
    "# Divide the data into train, test sets\n",
    "x_train = x[test_indicator == 0, :]\n",
    "x_test = x[test_indicator == 1, :]\n",
    "y_train = y[test_indicator == 0]\n",
    "y_test = y[test_indicator == 1]\n",
    "\n",
    "# Standardize the data.\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "# Keep track of the number of samples and dimension of each sample\n",
    "n_train = len(y_train)\n",
    "n_test = len(y_test)\n",
    "d = np.size(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions from previous homeworks\n",
    "def computegrad(beta, lambduh, x, y):\n",
    "    return -2/len(y)*x.T.dot(y-np.dot(x, beta)) + 2*lambduh*beta\n",
    "\n",
    "def graddescent(beta_init, eta, lambduh, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Run gradient descent with a fixed step size\n",
    "    Inputs:\n",
    "    - beta_init: Starting point\n",
    "    - eta: Step size (a constant)\n",
    "    - max_iter: Maximum number of iterations to perform\n",
    "    Output:\n",
    "    - beta_vals: Matrix of estimated betas at each iteration,\n",
    "    with the most recent values in the last row.\n",
    "    \"\"\"\n",
    "    beta = beta_init\n",
    "    grad_beta = computegrad(beta, lambduh)\n",
    "    beta_vals = [beta]\n",
    "    iter = 0\n",
    "    while iter < max_iter:\n",
    "        beta = beta - eta*grad_beta\n",
    "        beta_vals.append(beta)\n",
    "        grad_beta = computegrad(beta, lambduh)\n",
    "        iter += 1\n",
    "    return np.array(beta_vals)\n",
    "\n",
    "def obj(beta, lambduh, x, y):\n",
    "    return 1/len(y)*sum((y-x.dot(beta))**2) + lambduh*np.linalg.norm(beta)**2\n",
    "\n",
    "def convergence_plots(x_vals, lambduh):\n",
    "    \"\"\"\n",
    "    Plot the convergence in terms of the function values and the gradients\n",
    "    Input:\n",
    "        - x_vals: Values the gradient descent algorithm stepped to\n",
    "    \"\"\"\n",
    "    n, d = x_vals.shape\n",
    "    fs = np.zeros(n)\n",
    "    grads = np.zeros((n, d))\n",
    "    for i in range(n):\n",
    "        fs[i] = obj(x_vals[i], lambduh)\n",
    "        grads[i, :] = computegrad(x_vals[i], lambduh)\n",
    "    grad_norms = np.linalg.norm(grads, axis=1)\n",
    "    plt.subplot(121)\n",
    "    plt.plot(fs)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Objective value')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(grad_norms)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Norm of gradient')\n",
    "    plt.suptitle('Function Value and Norm of Gradient Convergence', fontsize=16)\n",
    "    plt.subplots_adjust(left=0.2, wspace=0.8, top=0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f2edc3a0db60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[0mlambduh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mbeta_init\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mbetas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraddescent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambduh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "eta = 0.05\n",
    "max_iter = 1000\n",
    "lambduh = 0.05\n",
    "d = X_train.shape[1]\n",
    "beta_init = np.random.normal(size=d)\n",
    "betas = graddescent(beta_init, eta, lambduh, max_iter=1000)\n",
    "convergence_plots(betas, lambduh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hw3\n",
    "def computegrad(beta, lambduh, x, y):\n",
    "    yx = y[:, np.newaxis]*x\n",
    "    denom = 1+np.exp(-yx.dot(beta))\n",
    "    grad = 1/len(y)*np.sum(-yx*np.exp(-yx.dot(beta[:, np.newaxis]))/\n",
    "        denom[:, np.newaxis], axis=0) + 2*lambduh*beta\n",
    "    return grad\n",
    "\n",
    "def objective(beta, lambduh, x, y):\n",
    "    return 1/len(y) * np.sum(np.log(1 + np.exp(-y*x.dot(beta)))) + lambduh * np.linalg.norm(beta)**2\n",
    "\n",
    "def backtracking(beta, lambduh, x, y, eta=1, alpha=0.5, betaparam=0.8, maxiter=100):\n",
    "    grad_beta = computegrad(beta, lambduh, x=x, y=y)\n",
    "    norm_grad_beta = np.linalg.norm(grad_beta)\n",
    "    found_eta = 0\n",
    "    iter = 0\n",
    "    while found_eta == 0 and iter < maxiter:\n",
    "        if objective(beta - eta * grad_beta, lambduh, x=x, y=y) < \\\n",
    "            objective(beta, lambduh, x=x, y=y)- alpha * eta * norm_grad_beta ** 2:\n",
    "                found_eta = 1\n",
    "        elif iter == maxiter:\n",
    "            raise ('Max number of iterations of backtracking line search reached')\n",
    "        else:\n",
    "            eta *= betaparam\n",
    "            iter += 1\n",
    "    return eta\n",
    "\n",
    "def graddescent(beta_init, lambduh, eta_init, x, y, eps=1e-4):\n",
    "    beta = beta_init\n",
    "    grad_beta = computegrad(beta, lambduh, x=x, y=y)\n",
    "    beta_vals = beta\n",
    "    iter = 0\n",
    "    while np.linalg.norm(grad_beta) > eps:\n",
    "        eta = backtracking(beta, lambduh, eta=eta_init, x=x, y=y)\n",
    "        beta = beta - eta*grad_beta\n",
    "        # Store all of the places we step to\n",
    "        beta_vals = np.vstack((beta_vals, beta))\n",
    "        grad_beta = computegrad(beta, lambduh, x=x, y=y)\n",
    "        iter += 1\n",
    "    return beta_vals\n",
    "\n",
    "def fastgradalgo(beta_init, theta_init, lambduh, eta_init, x, y, eps=1e-4):\n",
    "    beta = beta_init\n",
    "    theta = theta_init\n",
    "    grad_theta = computegrad(theta, lambduh, x=x, y=y)\n",
    "    grad_beta = computegrad(beta, lambduh, x=x, y=y)\n",
    "    beta_vals = beta\n",
    "    theta_vals = theta\n",
    "    iter = 0\n",
    "    while np.linalg.norm(grad_beta) > eps:\n",
    "        eta = backtracking(theta, lambduh, eta=eta_init, x=x, y=y)\n",
    "        beta_new = theta - eta*grad_theta\n",
    "        theta = beta_new + iter/(iter+3)*(beta_new-beta)\n",
    "        # Store all of the places we step to\n",
    "        beta_vals = np.vstack((beta_vals, beta))\n",
    "        theta_vals = np.vstack((theta_vals, theta))\n",
    "        grad_theta = computegrad(theta, lambduh, x=x, y=y)\n",
    "        grad_beta = computegrad(beta, lambduh, x=x, y=y)\n",
    "        beta = beta_new\n",
    "        iter += 1\n",
    "    return beta_vals\n",
    "\n",
    "def compute_misclassification_error(beta_opt, x, y):\n",
    "    y_pred = 1/(1+np.exp(-x.dot(beta_opt))) > 0.5\n",
    "    y_pred = y_pred*2 - 1 # Convert to +/- 1\n",
    "    return np.mean(y_pred != y)\n",
    "\n",
    "def plot_misclassification_error(betas_grad, betas_fastgrad, x, y, save_file='', title=''):\n",
    "    niter_grad = np.size(betas_grad, 0)\n",
    "    error_grad = np.zeros(niter_grad)\n",
    "    niter_fg = np.size(betas_fastgrad, 0)\n",
    "    error_fastgrad = np.zeros(niter_fg)\n",
    "    for i in range(niter_grad):\n",
    "        error_grad[i] = compute_misclassification_error(betas_grad[i, :], x, y)\n",
    "    for i in range(niter_fg):\n",
    "        error_fastgrad[i] = compute_misclassification_error(betas_fastgrad[i, :], x, y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(1, niter_grad + 1), error_grad, label='gradient descent')\n",
    "    ax.plot(range(1, niter_fg + 1), error_fastgrad, c='red', label='fast gradient')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Misclassification error')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    ax.legend(loc='upper right')\n",
    "    if not save_file:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Write a function _myrhologistic_ that implements the accelerated gradient algorithm to train the $\\ell_2^2$-regularized binary logistic regression with $\\rho$-logistic loss. The function takes as input\n",
    "the initial step-size for the backtracking rule, the $\\epsilon$ for the stopping criterion based on thenorm of the gradient of the objective, and the value of $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Train you $\\ell_2^2$-regularized binary logistic regression with $\\rho$-logistic loss with $\\rho=2$ and $\\epsilon=10^{-3}$ o the Spam dataset for $\\lambda=1$. Report your misclassification error for this value of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Write a function _crossval_ that implements leave-one-out cross-validation and hold-out cross-validation. You may either write a function that implements each variant separately depinging on the case, or write a general cross-validation function that can be instantiated in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Find the optimal value of $\\lambda$ using leave-one-out cross-validation.  Find the optimal value of $\\lambda$ useing hold-out cross-validation with a 80%/20% split for the training set/testing set.  Report your misclassification errors for the two values of $\\lambda$ found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Data Competition Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Pick two classes of your choice from the dataset. Train a classifier using $\\ell_2^2$-regularized binary\n",
    "logistic regression with $\\rho$-logistic loss on the training set using your own accelerated gradient algorithm with $\\rho = 2$, $\\epsilon = 10^{−3}$, and $\\lambda = 1$. Be sure to use the features you previously generated with the provided script rather than the raw image features. Plot, with different colors, the misclassification error on the training set and on the validation set vs iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Find the value of the regularization parameter $\\lambda$ using using leave-one-out cross-validation. Find the value of the regularization parameter $\\lambda$ using using hold-out cross-validation. Train a classifier using $\\ell_2^2$-regularized binary logistic regression with $\\rho$-logistic loss on the training set using your own accelerated gradient algorithm with that value of $\\lambda$ found by hold-out cross-validation. Plot, with different colors, the misclassification error on the training set and on the validation set vs. iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Consider all pairs of classes from the dataset. For each pair of classes, train a classifier using\n",
    "a $\\ell_2^2$-regularized binary logistic regression with $\\rho$-logistic loss on the training set comprising\n",
    "only the data-points for that pair of classes using your own fast gradient algorithm. For each\n",
    "pair of classes, find the value of the regularization parameter $\\lambda$ using hold-out cross-validation\n",
    "on the training set comprising only the data-points for that pair of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**  \n",
    "Write a function that for any new data point predicts its label. To do this, you will perform the following: input the data point into each classifier (for each pair of classes) you trained above. Record the class predicted by each classifier. Then your prediction for this data point is the most frequently predicted class. If there is a tie, randomly choose between the tied classes. Report the misclassification error on the validation set and test set. Report the precision/recall on the validation set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
