{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Kernel SVMs </center>\n",
    "<center> Corinne Jones </center>\n",
    "<center> DATA 558, Spring 2019 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll discuss kernels and how they can be used in conjunction with support vector machines. By the end of this lab, you should know:\n",
    "- What a kernel is and why we use them\n",
    "- What a kernel SVM is and how to implement one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels may be intuitively thought of as similarity measures. They are functions that take two inputs from a set $\\mathcal{X}$ and output a real number $\\mathbb{R}$. Below are several examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. RBF kernel  \n",
    "The radial basis function (RBF) kernel is given by\n",
    "$$ k(x,y) = \\exp\\left(-\\frac{1}{2\\sigma^2}\\|x-y\\|^2\\right),$$\n",
    "where $\\sigma$ is a parameter called the bandwidth that needs to be set. If we fix an element $x=1$ and picture what happens in the one-dimensional case when $\\sigma=0.5$, we get the picture below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = 1\n",
    "sigma=1\n",
    "y = np.arange(-5, 5, 0.1)\n",
    "K = np.exp(-1/sigma**2*(x-y)**2)\n",
    "plt.plot(y, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that $k(x,y)$ is largest when $y=x$ (i.e., when they are most similar) and that as $y$ becomes farther away from $x$ the value of the $k(x,y)$ decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Hellinger kernel\n",
    "The Hellinger kernel is given by $k(x,y) = \\sum_{i=1}^d\\sqrt{x_iy_i}$ for $x,y\\in\\mathbb{R}^d$ and is often used on normalized histograms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. A kernel for text\n",
    "Let $x,x'$ be two texts. Define $\\phi$ to be a mapping from a text to a vector with counts of the number of times every word in the English language appears in the text. Then $k(x,x')= \\phi(x)^T\\phi(x')$ is a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More technically, let $\\mathcal{X}$ be a non-empty set. A function $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ is called a kernel if there exists a Hilbert space $\\mathcal{H}$ and a map $\\phi:\\mathcal{X}\\to\\mathcal{H}$ such that for all $x,x'\\in\\mathcal{X}$,\n",
    "$$ k(x,x') = \\langle \\phi(x), \\phi(x')\\rangle_\\mathcal{H}.$$\n",
    "The main point here is that $k$ can be written as an inner product. You can think of a kernel as preprocessing observations $x$ and $x'$ by applying $\\phi$ and then taking the inner product of $\\phi(x)$ and $\\phi(x')$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel-based methods can control the smoothness of decision functions and help avoid underfitting and overfitting. Suppose a linear model is not appropriate for a given dataset. Then we could consider first applying a nonlinear function $\\phi$ to the datapoints and then fitting a linear classifier on the transformed features. Unlike with other methods, with kernel methods the features generated via such a $\\phi$ are often infinite-dimensional. While it may seem like infinite-dimensional features would be impossible to handle, if all we need is to evaluate inner products, then we can just rely on evaluating the kernel rather than ever worrying about (or even knowing) $\\phi$. This will be made more concrete in the next section.\n",
    "\n",
    "A note on terminology: Let $x_1,\\dots, x_n\\in\\mathcal{X}$ be a set of observations and $k:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ be a kernel. The matrix given by\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "k(x_1,x_1) & k(x_1,x_2) & \\cdots & k(x_1,x_n) \\\\\n",
    "k(x_2,x_1) & k(x_2,x_2) & \\cdots & k(x_2,x_n) \\\\\n",
    "\\vdots & & \\ddots & \\vdots\\\\\n",
    "k(x_n,x_1) & k(x_n,x_2) & \\cdots & k(x_n,x_n)\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "or alternatively written as $[k(x_i,x_j)]_{i,j=1}^n$ is called the Gram matrix and henceforth will be denoted by $K$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an algorithm relies on only dot products between features, then it can be \"kernelized\", i.e., we can use kernels with it. This is the case for many common machine learning methods, such as ridge regression, SVMs, PCA, CCA, $k$-means,... Here we'll examine the kernel SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Formulation\n",
    "Suppose we have inputs $x_1,\\dots, x_n\\in\\mathbb{R}^d$ and corresponding labels $y_1,\\dots, y_n\\in\\{-1, +1\\}$. Recall that the linear SVM with the squared hinge loss is given by\n",
    "$$ \\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n \\left\\{\\max\\left(0, 1-y_iw^Tx_i\\right)\\right\\}^2 + \\lambda\\|w\\|^2_2$$\n",
    "or equivalently,\n",
    "$$ \\min_{w\\in\\mathbb{R}^d} \\frac{1}{n}\\sum_{i=1}^n \\left\\{\\max\\left(0, 1-y_i\\langle w, x_i\\rangle_{\\mathbb{R}^d}\\right)\\right\\}^2 + \\lambda\\|w\\|^2_2. \\hspace{3cm} \\text{(1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letting $\\beta=(\\beta_1, \\dots, \\beta_n)^T$ and $K_i=(K(x_1, x_i),K(x_2,x_i),\\dots, K(x_n,x_i))^T$, the kernelized version of the above objective is\n",
    "\\begin{align}\n",
    "\\min_{\\beta\\in\\mathbb{R}^n} \\frac{1}{n}\\sum_{i=1}^n \\left\\{\\max\\left(0, 1-y_i\\beta^TK_i\\right)\\right\\}^2 + \\lambda \\beta^TK\\beta.  \\hspace{3cm} \\text{(2)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that we're optimizing over $\\beta\\in\\mathbb{R}^n$, i.e., the dimension of the vector we're optimizing over is the number of observations in the dataset rather than the dimension of the data.\n",
    "\n",
    "We can optimize this objective via (fast) gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** Compute the gradient of the objective function in equation (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're capable of optimizing this objective function. But once we do, how do we compute the prediction for a new point $x\\in\\mathbb{R}^d$? Recall that in a linear SVM we label a point $x$ as +1 if $w^Tx>0$ and as -1 otherwise. Here we analogously label a point as +1 if $\\sum_{i=1}^n \\beta_ik(x_i, x)>0$ and -1 otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Do-it-yourself kernel SVM\n",
    "Now we're ready to try it! We'll solve the problem using fast gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we code the objective function for the kernel SVM given in equation (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(beta, K, y, lam):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - beta: Vector to be optimized\n",
    "    - K: Gram matrix consisting of evaluations of the kernel k(x_i, x_j) for i,j=1,...,n\n",
    "    - y: Labels y_1,...,y_n corresponding to x_1,...,x_n\n",
    "    - lam: Penalty parameter lambda\n",
    "    Output:\n",
    "    - Value of the objective function at beta\n",
    "    \"\"\"\n",
    "    return np.mean(np.maximum(0, 1-y*K.dot(beta))**2) + lam*beta.dot(K).dot(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we code the gradient of the objective function for the kernel SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(beta, K, y, lam):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - beta: Vector to be optimized\n",
    "    - K: Gram matrix consisting of evaluations of the kernel k(x_i, x_j) for i,j=1,...,n\n",
    "    - y: Labels y_1,...,y_n corresponding to x_1,...,x_n\n",
    "    - lam: Penalty parameter lambda\n",
    "    Output:\n",
    "    - Value of the gradient at beta\n",
    "    \"\"\"\n",
    "    return -2*np.mean(np.maximum(0, 1-y*K.dot(beta))*y*K, axis=1) + 2*lam*K.dot(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "def test_grad(eps=1e-6):\n",
    "    n = 5\n",
    "    for i in range(n):\n",
    "        np.random.seed(0)\n",
    "        beta = -np.random.normal(size=n)\n",
    "        x = np.random.randn(n, n)\n",
    "        K = sklearn.metrics.pairwise.rbf_kernel(x, x)\n",
    "        y = np.random.choice([0, 1], size=5)\n",
    "        lam = 0.5\n",
    "        f1 = obj(beta, K, y, lam)\n",
    "        beta[i] = beta[i] + eps\n",
    "        f2 = obj(beta, K, y, lam)\n",
    "        print('Estimated and calculated values of beta[', i, ']:', (f2-f1)/eps, grad(beta, K, y, lam)[i])\n",
    "        assert np.isclose((f2-f1)/eps, grad(beta, K, y, lam)[i]), 'Estimated gradient ' + str((f2-f1)/eps) + ' is not approximately equal to the computed gradient ' + str(grad(beta, K, y, lam)[i])\n",
    "    print('Test passed')\n",
    "  \n",
    "test_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you've coded the fast gradient method yourself before, we will use my code for it in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bt_line_search(beta, K, y, lam, eta=1, alpha=0.5, betaparam=0.8, maxiter=100):\n",
    "    grad_beta = grad(beta, K, y, lam)\n",
    "    norm_grad_beta = np.linalg.norm(grad_beta)\n",
    "    found_eta = 0\n",
    "    iter = 0\n",
    "    while found_eta == 0 and iter < maxiter:\n",
    "        if obj(beta - eta * grad_beta, K, y, lam) < \\\n",
    "                        obj(beta, K, y, lam) - alpha * eta * norm_grad_beta ** 2:\n",
    "            found_eta = 1\n",
    "        elif iter == maxiter-1:\n",
    "            raise ('Max number of iterations of backtracking line search reached')\n",
    "        else:\n",
    "            eta *= betaparam\n",
    "            iter += 1\n",
    "    return eta\n",
    "\n",
    "def fastgradalgo(beta_init, theta_init, K, y, lam, eta_init, maxiter, eps=1e-5):\n",
    "    beta = beta_init\n",
    "    theta = theta_init\n",
    "    eta = eta_init\n",
    "    grad_theta = grad(theta, K, y, lam)\n",
    "    grad_beta = grad(beta, K, y, lam)\n",
    "    beta_vals = beta\n",
    "    iter = 0\n",
    "    while iter < maxiter and np.linalg.norm(grad_beta) > eps:\n",
    "        eta = bt_line_search(theta, K, y, lam, eta=eta)\n",
    "        beta_new = theta - eta*grad_theta\n",
    "        theta = beta_new + iter/(iter+3)*(beta_new-beta)\n",
    "        grad_theta = grad(theta, K, y, lam)\n",
    "        grad_beta = grad(beta, K, y, lam)\n",
    "        beta = beta_new\n",
    "        iter += 1\n",
    "        if iter % 1 == 0:\n",
    "            beta_vals = np.vstack((beta_vals, beta_new))\n",
    "    return beta_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2** We'll use the RBF kernel here. Recall that the RBF kernel is given by\n",
    "$$ k(x,y) = \\exp\\left(-\\frac{1}{2\\sigma^2}\\|x-y\\|^2\\right).$$\n",
    "Fill in the code to evaluate the kernel on matrices of observations of $X\\in\\mathbb{R}^{n\\times d}$ and $Z\\in\\mathbb{R}^{m\\times d}$. It should return $k(x_i,z_j)_{i,j=1}^{n,m}$. Code it yourself; don't use scikit-learn. Then test it with the scikit-learn code I provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(X, Z=None, sigma=0.5):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    - X: matrix with observations as rows\n",
    "    - Z: Another matrix with observations as rows\n",
    "    - Sigma: kernel bandwidth\n",
    "    Output: Gram matrix\n",
    "    \"\"\"  \n",
    "    if Z is None:\n",
    "        Z = X\n",
    "    gram = None\n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gram():  \n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(6, 5)\n",
    "    Z = np.random.randn(7, 5)\n",
    "    sigma = 10\n",
    "    gram1 = gram(X, Z, sigma)\n",
    "    gram2 = sklearn.metrics.pairwise.rbf_kernel(X, Z, gamma=1/(2*sigma**2))\n",
    "    assert np.allclose(gram1, gram2), 'Computed matrix' + str(gram1) + 'does not match that of scikit-learn:' + str(gram2)\n",
    "    print('Test passed')\n",
    "    \n",
    "test_gram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Artificial data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider an artificial dataset so we can visualize what the classifier ends up doing. Below you can see the data. The training points are solid colors and the test points are semi-transparent.  \n",
    "Credits: Some of this code was taken and modified from http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = sklearn.datasets.make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
    "y_train = 2*y_train - 1\n",
    "y_test = 2*y_test - 1\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "h = .02  # step size in the mesh\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "# Plot the training points\n",
    "ax = plt.subplot()\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "           edgecolors='k')\n",
    "# and testing points\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.1,\n",
    "           edgecolors='k')\n",
    "ax.set_xlim(xx.min(), xx.max())\n",
    "ax.set_ylim(yy.min(), yy.max())\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_plot(betas, K, y, lam, save_file=''):\n",
    "    num_points = np.size(betas, 0)\n",
    "    objs = np.zeros(num_points)\n",
    "    for i in range(0, num_points):\n",
    "        objs[i] = obj(betas[i, :], K, y, lam)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.array(range(num_points)), objs, c='red')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Objective value')\n",
    "    plt.title('Objective value vs. iteration when lambda=' + str(lam))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're finally ready to try out our SVM code. We'll use a rule-of-thumb for the bandwidth of the RBF kernel. The rule-of-thumb says to use the median distance between all pairs of points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "\n",
    "# Initialize things\n",
    "n, d = X_train.shape\n",
    "lam = 1\n",
    "beta_init = np.zeros(n)\n",
    "theta_init = np.zeros(n)\n",
    "dists = sklearn.metrics.pairwise.pairwise_distances(X_train).reshape(-1)\n",
    "sigma = np.median(dists)\n",
    "print('sigma=', sigma)\n",
    "K = gram(X_train, X_train, sigma)\n",
    "# Set eta_init based on an upper bound on the Lipschitz constant\n",
    "eta_init = 1 / scipy.linalg.eigh(2 / n * np.dot(K, K) + 2 * lam * K, eigvals=(n - 1, n - 1),\n",
    "                                 eigvals_only=True)[0]\n",
    "maxiter = 100\n",
    "\n",
    "# Run the algorithm\n",
    "betas = fastgradalgo(beta_init, theta_init, K, y_train, lam, eta_init, maxiter)\n",
    "\n",
    "# Examine results\n",
    "print('Optimal betas:', betas[-1, :])\n",
    "objective_plot(betas, K, y_train, lam, save_file='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it works! Now what is the misclassification error, and what does the decision boundary look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** Fill in the code below to compute the misclassification error. Recall that our prediction for a new point $x$ is $$\\text{sign}\\left(\\sum_{i=1}^n \\beta_ik(x_i, x)\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misclassification_error(beta, X_train, X_test, y_test, kernel, **kwargs):\n",
    "    n_test = len(y_test)\n",
    "    y_pred = np.zeros(n_test)\n",
    "    y_vals = np.zeros(n_test)\n",
    "    # To do: Change the argument of range from 1 to the appropriate value\n",
    "    for i in range(1):\n",
    "        if 'sigma' in kwargs:\n",
    "            sigma = kwargs['sigma']\n",
    "            y_vals[i] = np.dot(kernel(X_train, X_test[i, :].reshape(1, -1), sigma).reshape(-1), beta)\n",
    "        else:\n",
    "            y_vals[i] = np.dot(kernel(X_train, X_test[i, :].reshape(1, -1)).reshape(-1), beta)\n",
    "    # To do: Take the sign of y_vals to get y_pred\n",
    "    y_pred = None\n",
    "    return np.mean(y_pred != y_test), y_vals  # return error and values from before applying cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(betas, gram, sigma=None):\n",
    "    Zs = np.c_[xx.ravel(), yy.ravel()]\n",
    "    if sigma:\n",
    "        error, test_values = misclassification_error(betas[-1, :], X_train, X_test, y_test, gram, sigma=sigma)\n",
    "        Z = misclassification_error(betas[-1, :], X_train, Zs, [0]*len(Zs), gram, sigma=sigma)[1]\n",
    "    else:\n",
    "        error, test_values = misclassification_error(betas[-1, :], X_train, X_test, y_test, gram)\n",
    "        Z = misclassification_error(betas[-1, :], X_train, Zs, [0]*len(Zs), gram)[1]\n",
    "    print('Misclassification error when lambda =', lam, ':', error)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax = plt.subplot()\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n",
    "               edgecolors='k')\n",
    "    # and testing points\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n",
    "               edgecolors='k', alpha=0.2)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    plt.show()\n",
    "    \n",
    "evaluate(betas, gram, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the decision boundary is non-linear, as we would expect. Now let's compare to a non-kernel SVM. This is the same as using a kernel SVM with the linear kernel $K(x,y)=x^Ty$, so we can reuse our code from above and just modify the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** Fill in the code below to compute the linear kernel. Test your code using the code from scikit-learn below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_linear(X, Z=None):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    - X: matrix with observations as rows\n",
    "    - Z: Another matrix with observations as rows\n",
    "    Output: Gram matrix\n",
    "    \"\"\"  \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gram():  \n",
    "    np.random.seed(0)\n",
    "    X = np.random.randn(6, 5)\n",
    "    Z = np.random.randn(7, 5)\n",
    "    sigma = 10\n",
    "    gram1 = gram_linear(X, Z)\n",
    "    gram2 = sklearn.metrics.pairwise.linear_kernel(X, Z)\n",
    "    assert np.allclose(gram1, gram2), 'Computed matrix' + str(gram1) + 'does not match that of scikit-learn:' + str(gram2)\n",
    "    print('Test passed')\n",
    "    \n",
    "test_gram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the code again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize things\n",
    "n, d = X_train.shape\n",
    "lam = 1\n",
    "beta_init = np.zeros(n)\n",
    "theta_init = np.zeros(n)\n",
    "K = gram_linear(X_train, X_train)\n",
    "eta_init = 1 / scipy.linalg.eigh(2 / n * np.dot(K, K) + 2 * lam * K, eigvals=(n - 1, n - 1),\n",
    "                                 eigvals_only=True)[0]\n",
    "maxiter = 10\n",
    "\n",
    "# Run the algorithm\n",
    "betas = fastgradalgo(beta_init, theta_init, K, y_train, lam, eta_init, maxiter)\n",
    "\n",
    "# Examine results\n",
    "print('Optimal betas:', betas[-1, :])\n",
    "objective_plot(betas, K, y_train, lam, save_file='')\n",
    "evaluate(betas, gram_linear, sigma=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the decision boundary is linear, which is clearly inappropriate for this data. Using a kernel SVM makes a big difference!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
